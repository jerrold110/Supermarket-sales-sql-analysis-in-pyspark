{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "The purpose of this project is to analyse supermarket sales data through asking Key business questions such as \n",
    "'What product categories are performing well and not well at generating profits?' and 'Which of our branches are not performing well in Total sales and customer satisfaction?', then writing SQL queries to retrieve the data that answers those questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset metadata \n",
    "\n",
    "Attribute descriptions\n",
    "\n",
    "Invoice id: Computer generated sales slip invoice identification number\n",
    "\n",
    "Branch: Branch of supercenter (3 branches are available identified by A, B and C).\n",
    "\n",
    "City: Location of supercenters\n",
    "\n",
    "Customer type: Type of customers, recorded by Members for customers using member card and Normal for without member card.\n",
    "\n",
    "Gender: Gender type of customer\n",
    "\n",
    "Product line: General item categorization groups - Electronic accessories, Fashion accessories, Food and beverages, Health and beauty, Home and lifestyle, Sports and travel\n",
    "\n",
    "Unit price: Price of each product in $\n",
    "\n",
    "Quantity: Number of products purchased by customer\n",
    "\n",
    "Tax: 5% tax fee for customer buying\n",
    "\n",
    "Total: Total price including tax\n",
    "\n",
    "Date: Date of purchase (Record available from January 2019 to March 2019)\n",
    "\n",
    "Time: Purchase time (10am to 9pm)\n",
    "\n",
    "Payment: Payment used by customer for purchase (3 methods are available â€“ Cash, Credit card and Ewallet)\n",
    "\n",
    "COGS: Cost of goods sold\n",
    "\n",
    "Gross margin percentage: Gross margin percentage\n",
    "\n",
    "Gross income: Gross income\n",
    "\n",
    "Rating: Customer stratification rating on their overall shopping experience (On a scale of 1 to 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql.functions import col, unix_timestamp, to_date, to_timestamp, to_str, concat_ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = pyspark.sql.SparkSession.builder.appName(\"Sales queries\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://LAPTOP-R0JROKIO:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Sales queries</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1b0acee7610>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Records = 1000\n",
      "root\n",
      " |-- Invoice ID: string (nullable = true)\n",
      " |-- Branch: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Customer type: string (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Product line: string (nullable = true)\n",
      " |-- Unit price: string (nullable = true)\n",
      " |-- Quantity: string (nullable = true)\n",
      " |-- Tax 5%: string (nullable = true)\n",
      " |-- Total: string (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Time: string (nullable = true)\n",
      " |-- Payment: string (nullable = true)\n",
      " |-- cogs: string (nullable = true)\n",
      " |-- gross margin percentage: string (nullable = true)\n",
      " |-- gross income: string (nullable = true)\n",
      " |-- Rating: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_raw = spark.read.csv('data/supermarket_sales.csv', header=True, sep=\",\").cache()\n",
    "print('Total Records = {}'.format(df_raw.count()))\n",
    "df_raw.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+---------+-------------+------+--------------------+----------+--------+-------+--------+---------+-----+-----------+------+-----------------------+------------+------+\n",
      "| Invoice ID|Branch|     City|Customer type|Gender|        Product line|Unit price|Quantity| Tax 5%|   Total|     Date| Time|    Payment|  cogs|gross margin percentage|gross income|Rating|\n",
      "+-----------+------+---------+-------------+------+--------------------+----------+--------+-------+--------+---------+-----+-----------+------+-----------------------+------------+------+\n",
      "|750-67-8428|     A|   Yangon|       Member|Female|   Health and beauty|     74.69|       7|26.1415|548.9715| 1/5/2019|13:08|    Ewallet|522.83|            4.761904762|     26.1415|   9.1|\n",
      "|226-31-3081|     C|Naypyitaw|       Normal|Female|Electronic access...|     15.28|       5|   3.82|   80.22| 3/8/2019|10:29|       Cash|  76.4|            4.761904762|        3.82|   9.6|\n",
      "|631-41-3108|     A|   Yangon|       Normal|  Male|  Home and lifestyle|     46.33|       7|16.2155|340.5255| 3/3/2019|13:23|Credit card|324.31|            4.761904762|     16.2155|   7.4|\n",
      "|123-19-1176|     A|   Yangon|       Member|  Male|   Health and beauty|     58.22|       8| 23.288| 489.048|1/27/2019|20:33|    Ewallet|465.76|            4.761904762|      23.288|   8.4|\n",
      "|373-73-7910|     A|   Yangon|       Normal|  Male|   Sports and travel|     86.31|       7|30.2085|634.3785| 2/8/2019|10:37|    Ewallet|604.17|            4.761904762|     30.2085|   5.3|\n",
      "+-----------+------+---------+-------------+------+--------------------+----------+--------+-------+--------+---------+-----+-----------+------+-----------------------+------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_raw.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Date and Time columns to proper format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Column',\n",
       " 'DataFrame',\n",
       " 'DataType',\n",
       " 'PandasUDFType',\n",
       " 'PythonEvalType',\n",
       " 'SparkContext',\n",
       " 'StringType',\n",
       " 'UserDefinedFunction',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__spec__',\n",
       " '_create_column_from_literal',\n",
       " '_create_lambda',\n",
       " '_create_udf',\n",
       " '_get_get_jvm_function',\n",
       " '_get_lambda_parameters',\n",
       " '_invoke_binary_math_function',\n",
       " '_invoke_function',\n",
       " '_invoke_function_over_column',\n",
       " '_invoke_higher_order_function',\n",
       " '_options_to_str',\n",
       " '_test',\n",
       " '_to_java_column',\n",
       " '_to_seq',\n",
       " '_unresolved_named_lambda_variable',\n",
       " 'abs',\n",
       " 'acos',\n",
       " 'acosh',\n",
       " 'add_months',\n",
       " 'aggregate',\n",
       " 'approxCountDistinct',\n",
       " 'approx_count_distinct',\n",
       " 'array',\n",
       " 'array_contains',\n",
       " 'array_distinct',\n",
       " 'array_except',\n",
       " 'array_intersect',\n",
       " 'array_join',\n",
       " 'array_max',\n",
       " 'array_min',\n",
       " 'array_position',\n",
       " 'array_remove',\n",
       " 'array_repeat',\n",
       " 'array_sort',\n",
       " 'array_union',\n",
       " 'arrays_overlap',\n",
       " 'arrays_zip',\n",
       " 'asc',\n",
       " 'asc_nulls_first',\n",
       " 'asc_nulls_last',\n",
       " 'ascii',\n",
       " 'asin',\n",
       " 'asinh',\n",
       " 'assert_true',\n",
       " 'atan',\n",
       " 'atan2',\n",
       " 'atanh',\n",
       " 'avg',\n",
       " 'base64',\n",
       " 'bin',\n",
       " 'bitwiseNOT',\n",
       " 'broadcast',\n",
       " 'bround',\n",
       " 'bucket',\n",
       " 'cbrt',\n",
       " 'ceil',\n",
       " 'coalesce',\n",
       " 'col',\n",
       " 'collect_list',\n",
       " 'collect_set',\n",
       " 'column',\n",
       " 'concat',\n",
       " 'concat_ws',\n",
       " 'conv',\n",
       " 'corr',\n",
       " 'cos',\n",
       " 'cosh',\n",
       " 'count',\n",
       " 'countDistinct',\n",
       " 'covar_pop',\n",
       " 'covar_samp',\n",
       " 'crc32',\n",
       " 'create_map',\n",
       " 'cume_dist',\n",
       " 'current_date',\n",
       " 'current_timestamp',\n",
       " 'date_add',\n",
       " 'date_format',\n",
       " 'date_sub',\n",
       " 'date_trunc',\n",
       " 'datediff',\n",
       " 'dayofmonth',\n",
       " 'dayofweek',\n",
       " 'dayofyear',\n",
       " 'days',\n",
       " 'decode',\n",
       " 'degrees',\n",
       " 'dense_rank',\n",
       " 'desc',\n",
       " 'desc_nulls_first',\n",
       " 'desc_nulls_last',\n",
       " 'element_at',\n",
       " 'encode',\n",
       " 'exists',\n",
       " 'exp',\n",
       " 'explode',\n",
       " 'explode_outer',\n",
       " 'expm1',\n",
       " 'expr',\n",
       " 'factorial',\n",
       " 'filter',\n",
       " 'first',\n",
       " 'flatten',\n",
       " 'floor',\n",
       " 'forall',\n",
       " 'format_number',\n",
       " 'format_string',\n",
       " 'from_csv',\n",
       " 'from_json',\n",
       " 'from_unixtime',\n",
       " 'from_utc_timestamp',\n",
       " 'functools',\n",
       " 'get_json_object',\n",
       " 'greatest',\n",
       " 'grouping',\n",
       " 'grouping_id',\n",
       " 'hash',\n",
       " 'hex',\n",
       " 'hour',\n",
       " 'hours',\n",
       " 'hypot',\n",
       " 'initcap',\n",
       " 'input_file_name',\n",
       " 'instr',\n",
       " 'isnan',\n",
       " 'isnull',\n",
       " 'json_tuple',\n",
       " 'kurtosis',\n",
       " 'lag',\n",
       " 'last',\n",
       " 'last_day',\n",
       " 'lead',\n",
       " 'least',\n",
       " 'length',\n",
       " 'levenshtein',\n",
       " 'lit',\n",
       " 'locate',\n",
       " 'log',\n",
       " 'log10',\n",
       " 'log1p',\n",
       " 'log2',\n",
       " 'lower',\n",
       " 'lpad',\n",
       " 'ltrim',\n",
       " 'map_concat',\n",
       " 'map_entries',\n",
       " 'map_filter',\n",
       " 'map_from_arrays',\n",
       " 'map_from_entries',\n",
       " 'map_keys',\n",
       " 'map_values',\n",
       " 'map_zip_with',\n",
       " 'max',\n",
       " 'md5',\n",
       " 'mean',\n",
       " 'min',\n",
       " 'minute',\n",
       " 'monotonically_increasing_id',\n",
       " 'month',\n",
       " 'months',\n",
       " 'months_between',\n",
       " 'nanvl',\n",
       " 'next_day',\n",
       " 'nth_value',\n",
       " 'ntile',\n",
       " 'overlay',\n",
       " 'pandas_udf',\n",
       " 'percent_rank',\n",
       " 'percentile_approx',\n",
       " 'posexplode',\n",
       " 'posexplode_outer',\n",
       " 'pow',\n",
       " 'quarter',\n",
       " 'radians',\n",
       " 'raise_error',\n",
       " 'rand',\n",
       " 'randn',\n",
       " 'rank',\n",
       " 'regexp_extract',\n",
       " 'regexp_replace',\n",
       " 'repeat',\n",
       " 'reverse',\n",
       " 'rint',\n",
       " 'round',\n",
       " 'row_number',\n",
       " 'rpad',\n",
       " 'rtrim',\n",
       " 'schema_of_csv',\n",
       " 'schema_of_json',\n",
       " 'second',\n",
       " 'sequence',\n",
       " 'sha1',\n",
       " 'sha2',\n",
       " 'shiftLeft',\n",
       " 'shiftRight',\n",
       " 'shiftRightUnsigned',\n",
       " 'shuffle',\n",
       " 'signum',\n",
       " 'sin',\n",
       " 'since',\n",
       " 'sinh',\n",
       " 'size',\n",
       " 'skewness',\n",
       " 'slice',\n",
       " 'sort_array',\n",
       " 'soundex',\n",
       " 'spark_partition_id',\n",
       " 'split',\n",
       " 'sqrt',\n",
       " 'stddev',\n",
       " 'stddev_pop',\n",
       " 'stddev_samp',\n",
       " 'struct',\n",
       " 'substring',\n",
       " 'substring_index',\n",
       " 'sum',\n",
       " 'sumDistinct',\n",
       " 'sys',\n",
       " 'tan',\n",
       " 'tanh',\n",
       " 'timestamp_seconds',\n",
       " 'toDegrees',\n",
       " 'toRadians',\n",
       " 'to_csv',\n",
       " 'to_date',\n",
       " 'to_json',\n",
       " 'to_str',\n",
       " 'to_timestamp',\n",
       " 'to_utc_timestamp',\n",
       " 'transform',\n",
       " 'transform_keys',\n",
       " 'transform_values',\n",
       " 'translate',\n",
       " 'trim',\n",
       " 'trunc',\n",
       " 'udf',\n",
       " 'unbase64',\n",
       " 'unhex',\n",
       " 'unix_timestamp',\n",
       " 'upper',\n",
       " 'var_pop',\n",
       " 'var_samp',\n",
       " 'variance',\n",
       " 'warnings',\n",
       " 'weekofyear',\n",
       " 'when',\n",
       " 'window',\n",
       " 'xxhash64',\n",
       " 'year',\n",
       " 'years',\n",
       " 'zip_with']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(pyspark.sql.functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Invoice ID: string (nullable = true)\n",
      " |-- Branch: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Customer type: string (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Product line: string (nullable = true)\n",
      " |-- Unit price: string (nullable = true)\n",
      " |-- Quantity: string (nullable = true)\n",
      " |-- Tax 5%: string (nullable = true)\n",
      " |-- Total: string (nullable = true)\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Time: timestamp (nullable = true)\n",
      " |-- Payment: string (nullable = true)\n",
      " |-- cogs: string (nullable = true)\n",
      " |-- gross margin percentage: string (nullable = true)\n",
      " |-- gross income: string (nullable = true)\n",
      " |-- Rating: string (nullable = true)\n",
      "\n",
      "+-----------+------+---------+-------------+------+--------------------+----------+--------+-------+--------+----------+-------------------+-----------+------+-----------------------+------------+------+\n",
      "| Invoice ID|Branch|     City|Customer type|Gender|        Product line|Unit price|Quantity| Tax 5%|   Total|      Date|               Time|    Payment|  cogs|gross margin percentage|gross income|Rating|\n",
      "+-----------+------+---------+-------------+------+--------------------+----------+--------+-------+--------+----------+-------------------+-----------+------+-----------------------+------------+------+\n",
      "|750-67-8428|     A|   Yangon|       Member|Female|   Health and beauty|     74.69|       7|26.1415|548.9715|2019-01-05|2019-01-05 13:08:00|    Ewallet|522.83|            4.761904762|     26.1415|   9.1|\n",
      "|226-31-3081|     C|Naypyitaw|       Normal|Female|Electronic access...|     15.28|       5|   3.82|   80.22|2019-03-08|2019-03-08 10:29:00|       Cash|  76.4|            4.761904762|        3.82|   9.6|\n",
      "|631-41-3108|     A|   Yangon|       Normal|  Male|  Home and lifestyle|     46.33|       7|16.2155|340.5255|2019-03-03|2019-03-03 13:23:00|Credit card|324.31|            4.761904762|     16.2155|   7.4|\n",
      "|123-19-1176|     A|   Yangon|       Member|  Male|   Health and beauty|     58.22|       8| 23.288| 489.048|2019-01-27|2019-01-27 20:33:00|    Ewallet|465.76|            4.761904762|      23.288|   8.4|\n",
      "|373-73-7910|     A|   Yangon|       Normal|  Male|   Sports and travel|     86.31|       7|30.2085|634.3785|2019-02-08|2019-02-08 10:37:00|    Ewallet|604.17|            4.761904762|     30.2085|   5.3|\n",
      "+-----------+------+---------+-------------+------+--------------------+----------+--------+-------+--------+----------+-------------------+-----------+------+-----------------------+------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert Time column to timestamp after concatenating it with date\n",
    "# or else the default system date of 1970-01-01 will be used for the day\n",
    "df = df_raw.withColumn('Time', concat_ws('_','Date','Time'))\n",
    "df = df.withColumn('Time', \n",
    "                  to_timestamp(unix_timestamp(col('Time'), 'M/d/yyyy_HH:mm').cast(\"timestamp\")))\n",
    "\n",
    "# Convert Date column to timestamp\n",
    "df = df.withColumn('Date', \n",
    "                  to_date(unix_timestamp(col('Date'), 'M/d/yyyy').cast(\"timestamp\")))\n",
    "\n",
    "df.printSchema()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function for easy display of SQL query pyspark dataframes\n",
    "df.registerTempTable(\"sales\")\n",
    "\n",
    "def show_df(sqlquery):\n",
    "    spark.sql(sqlquery).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|        Product line|count|\n",
      "+--------------------+-----+\n",
      "|  Home and lifestyle|  160|\n",
      "| Fashion accessories|  178|\n",
      "|   Health and beauty|  152|\n",
      "|Electronic access...|  170|\n",
      "|  Food and beverages|  174|\n",
      "|   Sports and travel|  166|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sales per product line\n",
    "show_df(\"\"\"SELECT `Product line`,count(*) as count\n",
    "            FROM sales\n",
    "            GROUP BY `Product line`\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+\n",
      "|        Product line|total_sales|\n",
      "+--------------------+-----------+\n",
      "|  Home and lifestyle|   53861.91|\n",
      "| Fashion accessories|    54305.9|\n",
      "|   Health and beauty|   49193.74|\n",
      "|Electronic access...|   54337.53|\n",
      "|  Food and beverages|   56144.84|\n",
      "|   Sports and travel|   55122.83|\n",
      "+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Total sales per product line\n",
    "show_df(\"\"\"SELECT `Product line`, round(sum(`total`), 2) as total_sales\n",
    "           FROM sales\n",
    "           GROUP BY `Product line`\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|first_date| last_date|\n",
      "+----------+----------+\n",
      "|2019-01-01|2019-03-30|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "show_df('Select min(Date) as first_date,max(Date) as last_date from sales')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+\n",
      "|Branch|     City|\n",
      "+------+---------+\n",
      "|     A|   Yangon|\n",
      "|     B| Mandalay|\n",
      "|     C|Naypyitaw|\n",
      "+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "show_df('Select Branch, City from sales group by Branch, City order by Branch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+----------+\n",
      "|Branch|Gender|Avg_rating|\n",
      "+------+------+----------+\n",
      "|     A|  Male|     7.196|\n",
      "|     A|Female|     6.839|\n",
      "|     B|Female|     6.877|\n",
      "|     B|  Male|     6.762|\n",
      "|     C|  Male|     6.972|\n",
      "|     C|Female|     7.158|\n",
      "+------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Average rating by Branch and gender\n",
    "show_df('Select Branch, Gender, Round(avg(Rating),3) as Avg_rating from sales group by Branch, Gender order by Branch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select average gross margin percentage by product line, and total sales by product line and gender\n",
    "What product lines are under and over performing in terms of creating profit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+------------+\n",
      "|        Product line|Avg_margin_percent|total_profit|\n",
      "+--------------------+------------------+------------+\n",
      "|Electronic access...|              4.76|      2587.5|\n",
      "| Fashion accessories|              4.76|      2586.0|\n",
      "|  Food and beverages|              4.76|     2673.56|\n",
      "|   Health and beauty|              4.76|     2342.56|\n",
      "|  Home and lifestyle|              4.76|     2564.85|\n",
      "|   Sports and travel|              4.76|      2624.9|\n",
      "+--------------------+------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "show_df(\"\"\"SELECT `Product line`, round(avg(`gross margin percentage`),2) as Avg_margin_percent, \n",
    "            round(sum(`Gross income`),2) as total_profit\n",
    "           FROM sales\n",
    "           GROUP BY `Product line`\n",
    "           ORDER BY `Product line` ASC\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+-----------+\n",
      "|        Product line|Gender|total_sales|\n",
      "+--------------------+------+-----------+\n",
      "|Electronic access...|Female|    27102.0|\n",
      "|Electronic access...|  Male|    27236.0|\n",
      "| Fashion accessories|Female|    30437.0|\n",
      "| Fashion accessories|  Male|    23868.0|\n",
      "|  Food and beverages|Female|    33171.0|\n",
      "|  Food and beverages|  Male|    22974.0|\n",
      "|   Health and beauty|Female|    18561.0|\n",
      "|   Health and beauty|  Male|    30633.0|\n",
      "|  Home and lifestyle|Female|    30037.0|\n",
      "|  Home and lifestyle|  Male|    23825.0|\n",
      "|   Sports and travel|Female|    28575.0|\n",
      "|   Sports and travel|  Male|    26548.0|\n",
      "+--------------------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sales by product line and gender\n",
    "show_df(\"\"\"SELECT `Product line`, Gender, round(sum(`Total`)) as total_sales\n",
    "           FROM sales\n",
    "           GROUP BY `Product line`, Gender\n",
    "           ORDER BY  `Product line` ASC, Gender ASC\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Rating and Profit by Branch\n",
    "What  branches are successfully satisfying customers and successfully generating profit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+-----------------+\n",
      "|Branch|Avg_rating|     Total_profit|\n",
      "+------+----------+-----------------+\n",
      "|     A|     7.027|5057.160500000002|\n",
      "|     B|     6.818|5057.032000000003|\n",
      "|     C|     7.073|5265.176500000002|\n",
      "+------+----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output =  spark.sql(\"\"\"SELECT Branch, Round(avg(rating),3) as Avg_rating, sum(`gross income`) as Total_profit\n",
    "                        FROM sales\n",
    "                        GROUP BY Branch\n",
    "                        ORDER BY Branch ASC\"\"\")\n",
    "output.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Using Common Table Expression to get the Revenue by Branch for practice (even though there is no need to use it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+-----------------+------------------+\n",
      "|BRANCH|     Cost_of_goods|           profit|           Revenue|\n",
      "+------+------------------+-----------------+------------------+\n",
      "|     A|101143.21000000006|5057.160500000002|106200.37050000006|\n",
      "|     B|101140.63999999993|5057.032000000003|106197.67199999993|\n",
      "|     C|         105303.53|5265.176500000002|       110568.7065|\n",
      "+------+------------------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "show_df(\"\"\"WITH C as (SELECT Branch, sum(`Unit price` * Quantity) as Cogs\n",
    "                             FROM sales\n",
    "                             GROUP BY Branch),\n",
    "       P as (SELECT Branch, sum(`gross income`) as Profit\n",
    "                             FROM sales\n",
    "                             GROUP BY Branch)\n",
    "        SELECT C.BRANCH, C.Cogs as Cost_of_goods, P.profit, (C.cogs + P.Profit) as Revenue\n",
    "        FROM C \n",
    "        JOIN P ON C.Branch = P.Branch\n",
    "        ORDER BY C.BRANCH ASC\n",
    "        \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+\n",
      "|Branch|branch_sales|\n",
      "+------+------------+\n",
      "|     B|         332|\n",
      "|     C|         328|\n",
      "|     A|         340|\n",
      "+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "show_df(\"\"\"SELECT Branch, COUNT(Distinct(`Invoice ID`)) as branch_sales\n",
    "                                            FROM Sales \n",
    "                                            GROUP by Branch\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+---------------+-----------------------+\n",
      "|Branch|        Product line|Number of sales|Percent of branch sales|\n",
      "+------+--------------------+---------------+-----------------------+\n",
      "|     A|Electronic access...|             60|                  0.176|\n",
      "|     A| Fashion accessories|             51|                   0.15|\n",
      "|     A|  Food and beverages|             58|                  0.171|\n",
      "|     A|   Health and beauty|             47|                  0.138|\n",
      "|     A|  Home and lifestyle|             65|                  0.191|\n",
      "|     A|   Sports and travel|             59|                  0.174|\n",
      "|     B|Electronic access...|             55|                  0.166|\n",
      "|     B| Fashion accessories|             62|                  0.187|\n",
      "|     B|  Food and beverages|             50|                  0.151|\n",
      "|     B|   Health and beauty|             53|                   0.16|\n",
      "|     B|  Home and lifestyle|             50|                  0.151|\n",
      "|     B|   Sports and travel|             62|                  0.187|\n",
      "|     C|Electronic access...|             55|                  0.168|\n",
      "|     C| Fashion accessories|             65|                  0.198|\n",
      "|     C|  Food and beverages|             66|                  0.201|\n",
      "|     C|   Health and beauty|             52|                  0.159|\n",
      "|     C|  Home and lifestyle|             45|                  0.137|\n",
      "|     C|   Sports and travel|             45|                  0.137|\n",
      "+------+--------------------+---------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Is there a preference for certain product lines at certain branches?\n",
    "\n",
    "show_df(\"\"\"With BS as (SELECT Branch, COUNT(Distinct(`Invoice ID`)) as branch_sales\n",
    "                        FROM Sales \n",
    "                        GROUP by Branch  \n",
    "                            ),\n",
    "\n",
    "           PBS as (SELECT Branch, `Product line` as pl, Count(Distinct(`Invoice ID`)) as num_sales\n",
    "                    FROM sales\n",
    "                    GROUP BY Branch, `Product line`\n",
    "                    ORDER BY Branch ASC, `Product line`)\n",
    "                    \n",
    "            SELECT PBS.Branch, PBS.pl as `Product line`, PBS.num_sales as `Number of sales`, \n",
    "            Round(PBS.num_sales/BS.branch_sales, 3) as `Percent of branch sales`\n",
    "            FROM PBS join BS\n",
    "            ON PBS.Branch = Bs.Branch\n",
    "            ORDER BY Branch, `Product line`\n",
    "                        \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select by date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|      Date| Invoice ID|\n",
      "+----------+-----------+\n",
      "|2019-01-01|765-26-6951|\n",
      "|2019-01-01|530-90-9855|\n",
      "|2019-01-01|891-01-7034|\n",
      "|2019-01-01|493-65-6248|\n",
      "|2019-01-01|556-97-7101|\n",
      "|2019-01-01|133-14-7229|\n",
      "|2019-01-01|651-88-7328|\n",
      "|2019-01-01|182-52-7000|\n",
      "|2019-01-01|416-17-9926|\n",
      "|2019-01-01|271-77-8740|\n",
      "|2019-01-01|770-42-8960|\n",
      "|2019-01-01|746-04-1077|\n",
      "|2019-01-02|504-35-8843|\n",
      "|2019-01-02|446-47-6729|\n",
      "|2019-01-02|244-08-0162|\n",
      "|2019-01-02|198-84-7132|\n",
      "|2019-01-02|744-09-5786|\n",
      "|2019-01-02|712-39-0363|\n",
      "|2019-01-02|345-68-9016|\n",
      "|2019-01-02|670-71-7306|\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Invoices in the first 2 days\n",
    "show_df(\"\"\"SELECT Date, `Invoice ID`\n",
    "            FROM sales\n",
    "            WHERE DATE_TRUNC('day', Date) = '2019-01-01' or DATE_TRUNC('day', Date) = '2019-01-02'\n",
    "            SORT BY Date\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------+------------+\n",
      "|Branch|              Month|Gross_income|\n",
      "+------+-------------------+------------+\n",
      "|     A|2019-01-01 00:00:00|     1841.96|\n",
      "|     A|2019-02-01 00:00:00|     1421.91|\n",
      "|     A|2019-03-01 00:00:00|     1793.29|\n",
      "|     B|2019-01-01 00:00:00|     1770.29|\n",
      "|     B|2019-02-01 00:00:00|     1639.25|\n",
      "|     B|2019-03-01 00:00:00|     1647.49|\n",
      "|     C|2019-01-01 00:00:00|     1925.46|\n",
      "|     C|2019-02-01 00:00:00|     1568.33|\n",
      "|     C|2019-03-01 00:00:00|     1771.38|\n",
      "+------+-------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Monthly income by branch per month\n",
    "show_df(\"\"\"SELECT Branch, DATE_TRUNC('month', Date) as Month, Round(Sum(`Gross income`),2) as Gross_income \n",
    "            FROM sales\n",
    "            GROUP BY Branch, Month\n",
    "            ORDER BY Branch, Month\n",
    "            \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------+------------+\n",
      "|Branch|              Month|Gross_income|\n",
      "+------+-------------------+------------+\n",
      "|     A|2019-01-01 00:00:00|     1841.96|\n",
      "|     A|2019-02-01 00:00:00|     1421.91|\n",
      "|     B|2019-01-01 00:00:00|     1770.29|\n",
      "|     B|2019-02-01 00:00:00|     1639.25|\n",
      "|     C|2019-01-01 00:00:00|     1925.46|\n",
      "|     C|2019-02-01 00:00:00|     1568.33|\n",
      "+------+-------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Monthly income by branch for first 2 months\n",
    "show_df(\"\"\"SELECT Branch, DATE_TRUNC('month', Date) as Month, Round(Sum(`Gross income`),2) as Gross_income \n",
    "            FROM sales\n",
    "            WHERE DATE_TRUNC('Month', Date) = '2019-01-01' or DATE_TRUNC('Month', Date) = '2019-02-01'\n",
    "            GROUP BY Branch, Month\n",
    "            ORDER BY Branch, Month\n",
    "            \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+-----------------------------+\n",
      "|Day_of_week|Total_sales|Average_sales_per_transaction|\n",
      "+-----------+-----------+-----------------------------+\n",
      "|          1|   44457.89|                     334.2699|\n",
      "|          2|   37899.08|                     303.1926|\n",
      "|          3|   51482.25|                      325.837|\n",
      "|          4|   43731.14|                     305.8121|\n",
      "|          5|   45349.25|                     328.6177|\n",
      "|          6|   43926.34|                     316.0168|\n",
      "|          7|   56120.81|                     342.2001|\n",
      "+-----------+-----------+-----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# What days of the week have the most and least sales\n",
    "# Sunday is 1, Saturday is 7\n",
    "show_df(\"\"\"SELECT dayofweek(Date) as Day_of_week, Round(Sum(Total),2) as Total_sales, \n",
    "            Round(AVG(Total),4) as Average_sales_per_transaction\n",
    "            FROM sales\n",
    "            GROUP BY Day_of_week\n",
    "            ORDER BY Day_of_week\n",
    "            Limit 7\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running total sales, Lagged sales, Sales delta/growth by week\n",
    "### Analysing sales KPI by weeks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+\n",
      "|         Week_start|Sales|\n",
      "+-------------------+-----+\n",
      "|2018-12-31 00:00:00|   55|\n",
      "|2019-01-07 00:00:00|   73|\n",
      "|2019-01-14 00:00:00|   82|\n",
      "|2019-01-21 00:00:00|   93|\n",
      "|2019-01-28 00:00:00|   83|\n",
      "|2019-02-04 00:00:00|   92|\n",
      "|2019-02-11 00:00:00|   72|\n",
      "|2019-02-18 00:00:00|   60|\n",
      "|2019-02-25 00:00:00|   87|\n",
      "|2019-03-04 00:00:00|   88|\n",
      "|2019-03-11 00:00:00|   78|\n",
      "|2019-03-18 00:00:00|   76|\n",
      "|2019-03-25 00:00:00|   61|\n",
      "+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Weekly sales count\n",
    "show_df(\"\"\"SELECT DATE_TRUNC('week', Date) as Week_start, COUNT(*) as Sales\n",
    "            FROM sales\n",
    "            GROUP BY Week_start\n",
    "            ORDER BY Week_start ASC\n",
    "            \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+-------------+\n",
      "|         Week_start|sales|Running_sales|\n",
      "+-------------------+-----+-------------+\n",
      "|2018-12-31 00:00:00|   55|           55|\n",
      "|2019-01-07 00:00:00|   73|          128|\n",
      "|2019-01-14 00:00:00|   82|          210|\n",
      "|2019-01-21 00:00:00|   93|          303|\n",
      "|2019-01-28 00:00:00|   83|          386|\n",
      "|2019-02-04 00:00:00|   92|          478|\n",
      "|2019-02-11 00:00:00|   72|          550|\n",
      "|2019-02-18 00:00:00|   60|          610|\n",
      "|2019-02-25 00:00:00|   87|          697|\n",
      "|2019-03-04 00:00:00|   88|          785|\n",
      "|2019-03-11 00:00:00|   78|          863|\n",
      "|2019-03-18 00:00:00|   76|          939|\n",
      "|2019-03-25 00:00:00|   61|         1000|\n",
      "+-------------------+-----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Weekly running sales\n",
    "show_df(\"\"\"WITH weekly_sales as(SELECT DATE_TRUNC('week', Date) as Week_start, COUNT(*) as sales\n",
    "            FROM sales\n",
    "            GROUP BY Week_start\n",
    "            ORDER BY Week_start ASC)\n",
    "            \n",
    "            SELECT Week_start, sales, sum(sales) over (ORDER BY Week_start ASC) as Running_sales\n",
    "            FROM weekly_sales\n",
    "            ORDER BY Week_start ASC\n",
    "            \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+------------+\n",
      "|         Week_start|sales|Lagged_sales|\n",
      "+-------------------+-----+------------+\n",
      "|2018-12-31 00:00:00|   55|           0|\n",
      "|2019-01-07 00:00:00|   73|          55|\n",
      "|2019-01-14 00:00:00|   82|          73|\n",
      "|2019-01-21 00:00:00|   93|          82|\n",
      "|2019-01-28 00:00:00|   83|          93|\n",
      "|2019-02-04 00:00:00|   92|          83|\n",
      "|2019-02-11 00:00:00|   72|          92|\n",
      "|2019-02-18 00:00:00|   60|          72|\n",
      "|2019-02-25 00:00:00|   87|          60|\n",
      "|2019-03-04 00:00:00|   88|          87|\n",
      "|2019-03-11 00:00:00|   78|          88|\n",
      "|2019-03-18 00:00:00|   76|          78|\n",
      "|2019-03-25 00:00:00|   61|          76|\n",
      "+-------------------+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Weekly Lagged sales\n",
    "show_df(\"\"\"WITH weekly_sales as(SELECT DATE_TRUNC('week', Date) as Week_start, COUNT(*) as sales\n",
    "            FROM sales\n",
    "            GROUP BY Week_start\n",
    "            ORDER BY Week_start ASC)\n",
    "            \n",
    "            SELECT Week_start, sales, coalesce(lag(sales) over (ORDER BY Week_start ASC), 0) as Lagged_sales\n",
    "            FROM weekly_sales\n",
    "            ORDER BY Week_start ASC\n",
    "            \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+------------+\n",
      "|         Week_start|Sales_delta|Sales_growth|\n",
      "+-------------------+-----------+------------+\n",
      "|2018-12-31 00:00:00|          5|         0.1|\n",
      "|2019-01-07 00:00:00|         18|        0.33|\n",
      "|2019-01-14 00:00:00|          9|        0.12|\n",
      "|2019-01-21 00:00:00|         11|        0.13|\n",
      "|2019-01-28 00:00:00|        -10|       -0.11|\n",
      "|2019-02-04 00:00:00|          9|        0.11|\n",
      "|2019-02-11 00:00:00|        -20|       -0.22|\n",
      "|2019-02-18 00:00:00|        -12|       -0.17|\n",
      "|2019-02-25 00:00:00|         27|        0.45|\n",
      "|2019-03-04 00:00:00|          1|        0.01|\n",
      "|2019-03-11 00:00:00|        -10|       -0.11|\n",
      "|2019-03-18 00:00:00|         -2|       -0.03|\n",
      "|2019-03-25 00:00:00|        -15|        -0.2|\n",
      "+-------------------+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Changes in number of sales every week and changes in sales growth (Percent)\n",
    "# ASSUME THAT THERE WERE 50 SALES IN THE WEEK PRIOR TO THE WEEK THAT BEGAN ON 2019-12-31\n",
    "\n",
    "show_df(\"\"\"WITH weekly_sales as(SELECT DATE_TRUNC('week', Date) as Week_start, COUNT(*) as Sales\n",
    "            FROM sales\n",
    "            GROUP BY Week_start\n",
    "            ORDER BY Week_start ASC),\n",
    "            \n",
    "            weekly_sales_lag as(SELECT Week_start, sales, coalesce(lag(sales) over (ORDER BY Week_start ASC), 50) as Lagged_sales\n",
    "            FROM weekly_sales\n",
    "            ORDER BY Week_start ASC)\n",
    "            \n",
    "            SELECT Week_start, Sales-Lagged_sales as Sales_delta, round((Sales-Lagged_sales)/Lagged_sales,2) as Sales_growth\n",
    "            FROM weekly_sales_lag\n",
    "            ORDER BY Week_start ASC\n",
    "            \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------+--------------+\n",
      "|         Week_start|Revenue_delta|Revenue_growth|\n",
      "+-------------------+-------------+--------------+\n",
      "|2018-12-31 00:00:00|      7543.39|          0.75|\n",
      "|2019-01-07 00:00:00|      6917.81|          0.39|\n",
      "|2019-01-14 00:00:00|      4232.16|          0.17|\n",
      "|2019-01-21 00:00:00|       593.52|          0.02|\n",
      "|2019-01-28 00:00:00|      -926.44|         -0.03|\n",
      "|2019-02-04 00:00:00|     -1258.61|         -0.04|\n",
      "|2019-02-11 00:00:00|     -1538.24|         -0.06|\n",
      "|2019-02-18 00:00:00|     -8234.93|         -0.32|\n",
      "|2019-02-25 00:00:00|     11891.05|          0.69|\n",
      "|2019-03-04 00:00:00|      -800.88|         -0.03|\n",
      "|2019-03-11 00:00:00|     -4428.62|         -0.16|\n",
      "|2019-03-18 00:00:00|      1130.38|          0.05|\n",
      "|2019-03-25 00:00:00|      -7242.6|         -0.29|\n",
      "+-------------------+-------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Changes in number of sales every week and changes in sales growth (Percent)\n",
    "# Assume Income in prior week was 10000\n",
    "\n",
    "show_df(\"\"\"WITH weekly_sales as(SELECT DATE_TRUNC('week', Date) as Week_start, SUM(Total) as Sales\n",
    "            FROM sales\n",
    "            GROUP BY Week_start\n",
    "            ORDER BY Week_start ASC),\n",
    "            -- Assume income in prior week\n",
    "            weekly_sales_lag as(SELECT Week_start, Sales, coalesce(lag(Sales) over (ORDER BY Week_start ASC), 10000) as Lagged_sales\n",
    "            FROM weekly_sales\n",
    "            ORDER BY Week_start ASC)\n",
    "            \n",
    "            SELECT Week_start, Round(Sales-Lagged_sales,2) as Revenue_delta, round((Sales-Lagged_sales)/Lagged_sales,2) as Revenue_growth\n",
    "            FROM weekly_sales_lag\n",
    "            ORDER BY Week_start ASC\n",
    "            \"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
